# Spark ETL æ€§èƒ½åˆ†æè®­ç»ƒåœº (NYC Taxi Edition)

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå­¦ä¹  **Spark SQL ETL å¼€å‘**åŠ **Spark UI æ€§èƒ½è°ƒä¼˜**çš„å®éªŒç¯å¢ƒã€‚é€šè¿‡åœ¨çœŸå®åœºæ™¯ä¸­æ„é€ â€œåä»£ç â€ï¼Œå¤ç°å¹¶åˆ†ææ•°æ®å€¾æ–œã€æ•°æ®è†¨èƒ€ã€å¤æ‚ Shuffle ç­‰å¸¸è§çš„ç”Ÿäº§ç“¶é¢ˆã€‚

## ğŸš€ æŠ€æœ¯æ ˆ
*   **Spark:** 3.5.8
*   **Scala:** 2.12.18
*   **Hadoop:** 3.3.6 (YARN Mode)
*   **Dataset:** NYC Yellow Taxi Trip Records & Zone Lookup

---

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. ç¼–è¯‘æ‰“åŒ…ä¸å¯åŠ¨
ä½ å¯ä»¥é€‰æ‹©åˆ†æ­¥æ‰§è¡Œï¼Œæˆ–è€…ä½¿ç”¨é¢„è®¾çš„ Maven Profile ä¸€é”®å®Œæˆç¼–è¯‘ã€æ‹·è´ Jar åŒ…åŠé‡å¯ Docker ç¯å¢ƒã€‚

**ä¸€é”®å®Œæˆï¼ˆæ¨èï¼‰:**
```bash
mvn clean package -DskipTests -Prun
```

**åˆ†æ­¥æ‰§è¡Œ:**
```bash
# ä»…ç¼–è¯‘æ‰“åŒ… (è‡ªåŠ¨æ‹·è´ Jar åˆ° workspace)
mvn clean package -DskipTests

# æ‰‹åŠ¨å¯åŠ¨ç¯å¢ƒ
docker-compose down -v && docker-compose up -d
```

### 3. æŸ¥çœ‹è¿è¡Œæƒ…å†µ
ç¯å¢ƒå¯åŠ¨åï¼Œ`master` å®¹å™¨ä¼šè‡ªåŠ¨å°† `workspace/data` ä¸‹çš„æ•°æ®ä¸Šä¼ è‡³ HDFSï¼ŒåŒæ—¶ `spark` å®¹å™¨ä¼šè‡ªåŠ¨è§¦å‘ `spark-submit`ã€‚

ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹å®¹å™¨æ—¥å¿—æ¥è§‚å¯Ÿè¿è¡Œæƒ…å†µï¼š
```bash
docker logs -f spark
```

---

## ğŸ“Š ç›‘æ§ä¸åˆ†æ

é¡¹ç›®é›†æˆäº† Spark UI å’Œ History Serverï¼Œæ–¹ä¾¿è¿›è¡Œå…¨æ–¹ä½çš„æ€§èƒ½è¯Šæ–­ã€‚

*   **Spark UI (è¿è¡Œä¸­)**: [http://localhost:4040](http://localhost:4040)
*   **YARN ResourceManager**: [http://localhost:8088](http://localhost:8088)
*   **Spark History Server**: [http://localhost:18080](http://localhost:18080) *(ä»»åŠ¡å®ŒæˆåæŸ¥çœ‹æ—¥å¿—)*

---

## ğŸ” Spark UI å­¦ä¹ é‡ç‚¹

### Scenario 1: æ•°æ®è†¨èƒ€ä¸å®½è¡¨ (Data Inflation & Wide Table)
*   **ç°è±¡**: å°†åŸå§‹æ•°æ®ç¿» 10 å€ï¼Œå¹¶è¡ç”Ÿ 100 åˆ—å®½è¡¨ã€‚
*   **UI è§‚å¯Ÿç‚¹**:
    *   **Input Size / Records**: è§‚å¯Ÿ Stage 1 çš„æ•°æ®è¯»å–æ€»é‡ã€‚
    *   **Storage Tab**: è§‚å¯Ÿ `wideDF` åœ¨åºåˆ—åŒ–ç¼“å­˜åçš„å†…å­˜ä¸ç£ç›˜å ç”¨æƒ…å†µã€‚

### Scenario 2: å‰§çƒˆ Shuffle ä¸æ•°æ®å€¾æ–œ (Ultra Shuffle & Skew)
*   **ç°è±¡**: åœ¨å€¾æ–œå­—æ®µ `PULocationID` ä¸Šè¿›è¡Œæµ·é‡æ•°æ®çš„ Self-Joinã€‚
*   **UI è§‚å¯Ÿç‚¹**:
    *   **Shuffle Write/Read**: è§‚å¯Ÿé«˜è¾¾æ•° GB ç”šè‡³æ›´å¤šçš„ Shuffle æ•°æ®ä¼ è¾“ã€‚
    *   **Task Skew**: è§‚å¯Ÿç‰¹å®š Task å¤„ç†æ•°æ®é‡è¿œè¶…å…¶ä»– Task çš„ç°è±¡ã€‚

### Scenario 3: å¤æ‚ DAG ä¸ å¤šçº§èšåˆ (Complex DAG & Multi-Agg)
*   **ç°è±¡**: å…ˆå†™å‡º Parquet å†è¯»å›è¿›è¡Œå¤šç»´åº¦èšåˆæ’åºã€‚
*   **UI è§‚å¯Ÿç‚¹**:
    *   **DAG Visualization**: æŸ¥çœ‹å®Œæ•´çš„è¡€ç¼˜å…³ç³»å’Œ Stage åˆ’åˆ†ã€‚
    *   **Job Grouping**: åœ¨ UI ä¸­é€šè¿‡è‡ªå®šä¹‰çš„ Job Group Nameï¼ˆå¦‚ `Step_5_Final_Agg`ï¼‰å¿«é€Ÿå®šä½é€»è¾‘ã€‚

---

## ğŸ’¡ å®éªŒå»ºè®®
ä½ å¯ä»¥å°è¯•ä¿®æ”¹ `src/main/scala/com/example/spark/BadTaxiApp.scala` ä¸­çš„é…ç½®å¹¶é‡æ–°æ‰“åŒ…ï¼Œè§‚å¯Ÿ UI å˜åŒ–ï¼š
*   **ä¼˜åŒ– Join**: å¯ç”¨/ç¦ç”¨ `spark.sql.autoBroadcastJoinThreshold`ã€‚
*   **è°ƒæ•´å¹¶è¡Œåº¦**: ä¿®æ”¹ `spark.sql.shuffle.partitions`ï¼ˆå½“å‰è®¾ä¸º 10ï¼Œè§‚å¯Ÿæ”¹ä¸º 200 åçš„å˜åŒ–ï¼‰ã€‚
*   **å†…å­˜ç®¡ç†**: åœ¨ `workspace/start-spark.sh` ä¸­è°ƒæ•´ `--executor-memory`ï¼Œè§‚å¯Ÿå¯¹ Spill çš„å½±å“ã€‚
